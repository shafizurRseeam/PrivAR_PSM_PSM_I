{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98a7a5eb-b605-4630-a7a7-e2c1b06b203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # ============================================================\n",
    "# # USER INPUTS\n",
    "# # ============================================================\n",
    "# BASE_DIR   = r\"C:\\Users\\ss6365\\Desktop\\PrivAR_PSM_PSM_I\\data\\distorted_only_privacy\\geotrace\\delta_3_Noisy_threshold_5\\perturbed_1_copy_data_5\"\n",
    "# MECHS      = [\"plm\", \"psm\", \"psmi\"]\n",
    "# GRID_SIZE  = 40  # grid40 => 40x40 = 1600 location_ids\n",
    "\n",
    "# # Which columns to discretize (true lat/lon -> location_id)\n",
    "# LAT_COL = \"latitude\"\n",
    "# LON_COL = \"longitude\"\n",
    "\n",
    "# # Output column name\n",
    "# LOC_ID_COL = \"location_id\"\n",
    "\n",
    "# # If you want consistent discretization across ALL files in a mechanism folder,\n",
    "# # keep this True. (Recommended for ML: same lat/lon always maps to same id.)\n",
    "# GLOBAL_BOUNDS_PER_MECH = True\n",
    "\n",
    "# # ============================================================\n",
    "# # HELPERS\n",
    "# # ============================================================\n",
    "# def compute_bounds(csv_paths, lat_col, lon_col):\n",
    "#     \"\"\"Compute global min/max across many CSVs (robust: ignores NaNs).\"\"\"\n",
    "#     lat_min = np.inf\n",
    "#     lat_max = -np.inf\n",
    "#     lon_min = np.inf\n",
    "#     lon_max = -np.inf\n",
    "\n",
    "#     for p in csv_paths:\n",
    "#         df = pd.read_csv(p, usecols=[lat_col, lon_col])\n",
    "#         lat_min = min(lat_min, np.nanmin(df[lat_col].to_numpy()))\n",
    "#         lat_max = max(lat_max, np.nanmax(df[lat_col].to_numpy()))\n",
    "#         lon_min = min(lon_min, np.nanmin(df[lon_col].to_numpy()))\n",
    "#         lon_max = max(lon_max, np.nanmax(df[lon_col].to_numpy()))\n",
    "\n",
    "#     # Handle edge case: all values equal (avoid zero step)\n",
    "#     if lat_min == lat_max:\n",
    "#         lat_max = lat_min + 1e-12\n",
    "#     if lon_min == lon_max:\n",
    "#         lon_max = lon_min + 1e-12\n",
    "\n",
    "#     return lat_min, lat_max, lon_min, lon_max\n",
    "\n",
    "\n",
    "# def latlon_to_location_id(lat, lon, lat_min, lat_max, lon_min, lon_max, grid_size):\n",
    "#     \"\"\"\n",
    "#     Map (lat, lon) -> integer location_id in [1, grid_size^2]\n",
    "#     Uses uniform grid, clamps boundary points safely.\n",
    "#     \"\"\"\n",
    "#     lat_step = (lat_max - lat_min) / grid_size\n",
    "#     lon_step = (lon_max - lon_min) / grid_size\n",
    "\n",
    "#     lat_pos = np.floor((lat - lat_min) / lat_step).astype(int)\n",
    "#     lon_pos = np.floor((lon - lon_min) / lon_step).astype(int)\n",
    "\n",
    "#     # Clamp to [0, grid_size-1]\n",
    "#     lat_pos = np.clip(lat_pos, 0, grid_size - 1)\n",
    "#     lon_pos = np.clip(lon_pos, 0, grid_size - 1)\n",
    "\n",
    "#     # 2D -> 1D, +1 to make IDs start at 1\n",
    "#     return (lat_pos * grid_size + lon_pos + 1).astype(int)\n",
    "\n",
    "\n",
    "# def add_location_id(df, lat_col, lon_col, grid_size, bounds):\n",
    "#     lat_min, lat_max, lon_min, lon_max = bounds\n",
    "\n",
    "#     lat = df[lat_col].to_numpy(dtype=float)\n",
    "#     lon = df[lon_col].to_numpy(dtype=float)\n",
    "\n",
    "#     df[LOC_ID_COL] = latlon_to_location_id(lat, lon, lat_min, lat_max, lon_min, lon_max, grid_size)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # MAIN\n",
    "# # ============================================================\n",
    "# for mech in MECHS:\n",
    "#     mech_dir = os.path.join(BASE_DIR, mech)\n",
    "#     if not os.path.isdir(mech_dir):\n",
    "#         print(f\"[SKIP] Missing mech dir: {mech_dir}\")\n",
    "#         continue\n",
    "\n",
    "#     # Collect ALL csv files in mech folder (only files directly under plm/psm/psmi)\n",
    "#     # If you also want to include nested folders, switch to: glob.glob(mech_dir + \"/**/*.csv\", recursive=True)\n",
    "#     csv_paths = sorted(glob.glob(os.path.join(mech_dir, \"*.csv\")))\n",
    "\n",
    "#     if not csv_paths:\n",
    "#         print(f\"[SKIP] No CSVs in: {mech_dir}\")\n",
    "#         continue\n",
    "\n",
    "#     # Output folder: plm/grid40 (or psm/grid40, psmi/grid40)\n",
    "#     out_dir = os.path.join(mech_dir, f\"grid{GRID_SIZE}\")\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#     # Bounds: either global per mechanism (recommended) or per file\n",
    "#     if GLOBAL_BOUNDS_PER_MECH:\n",
    "#         bounds = compute_bounds(csv_paths, LAT_COL, LON_COL)\n",
    "#         print(f\"[INFO] {mech}: global bounds lat[{bounds[0]}, {bounds[1]}], lon[{bounds[2]}, {bounds[3]}]\")\n",
    "\n",
    "#     for p in csv_paths:\n",
    "#         df = pd.read_csv(p)\n",
    "\n",
    "#         # Basic safety checks\n",
    "#         if LAT_COL not in df.columns or LON_COL not in df.columns:\n",
    "#             print(f\"[SKIP] Missing {LAT_COL}/{LON_COL} in {p}\")\n",
    "#             continue\n",
    "\n",
    "#         if not GLOBAL_BOUNDS_PER_MECH:\n",
    "#             bounds = (df[LAT_COL].min(), df[LAT_COL].max(), df[LON_COL].min(), df[LON_COL].max())\n",
    "#             # avoid zero step\n",
    "#             if bounds[0] == bounds[1]:\n",
    "#                 bounds = (bounds[0], bounds[1] + 1e-12, bounds[2], bounds[3])\n",
    "#             if bounds[2] == bounds[3]:\n",
    "#                 bounds = (bounds[0], bounds[1], bounds[2], bounds[3] + 1e-12)\n",
    "\n",
    "#         df = add_location_id(df, LAT_COL, LON_COL, GRID_SIZE, bounds)\n",
    "\n",
    "#         base = os.path.splitext(os.path.basename(p))[0]\n",
    "#         out_name = f\"{base}_grid{GRID_SIZE}.csv\"\n",
    "#         out_path = os.path.join(out_dir, out_name)\n",
    "\n",
    "#         df.to_csv(out_path, index=False)\n",
    "#         print(f\"[OK] {mech}: {os.path.basename(p)} -> {out_name}\")\n",
    "\n",
    "# print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e735751a-aef2-4c29-9589-25dad3d4e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from collections import defaultdict\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # ============================================================\n",
    "# # CONFIG (EDIT THESE)\n",
    "# # ============================================================\n",
    "# PLM_CSV  = r\"C:\\Users\\ss6365\\Desktop\\PrivAR_PSM_PSM_I\\data\\distorted_only_privacy\\geotrace\\delta_3_Noisy_threshold_5\\perturbed_1_copy_data_5\\plm\\grid200\\merged_geotrace_plm_delta_3_Noisy_threshold_5_eps05_grid200.csv\"\n",
    "# PSM_CSV  = r\"C:\\Users\\ss6365\\Desktop\\PrivAR_PSM_PSM_I\\data\\distorted_only_privacy\\geotrace\\delta_3_Noisy_threshold_5\\perturbed_1_copy_data_5\\psm\\grid200\\merged_geotrace_psm_delta_3_Noisy_threshold_5_eps05_grid200.csv\"\n",
    "# PSMI_CSV = r\"C:\\Users\\ss6365\\Desktop\\PrivAR_PSM_PSM_I\\data\\distorted_only_privacy\\geotrace\\delta_3_Noisy_threshold_5\\perturbed_1_copy_data_5\\psmi\\grid200\\merged_geotrace_psmi_delta_3_Noisy_threshold_5_eps05_grid200.csv\"\n",
    "\n",
    "# # Column names in your merged CSV\n",
    "# ID_COL   = \"identifier\"  # trajectory/user id column (change if yours differs)\n",
    "# Y_COL    = \"location_id\"\n",
    "# X_COLS   = [\"perturbed_latitude\", \"perturbed_longitude\"]  # attacker observes these\n",
    "\n",
    "# # Evaluation settings\n",
    "# TEST_SIZE          = 0.20\n",
    "# RANDOM_SEED        = 42\n",
    "# MAX_TRACE_LEN_LIST = [1, 5]  # edit as you like\n",
    "# HISTORY_R          = 5      # r in your paper\n",
    "# ALPHA_DECAY        = 0.7    # exponential decay for weights w_j ~ alpha^(j-1)\n",
    "# LAPLACE_SMOOTH     = 1e-6   # smoothing for transition probs to avoid zeros\n",
    "# USE_STANDARDIZE    = True   # recommended for kNN on lat/lon\n",
    "\n",
    "# # ============================================================\n",
    "# # HELPERS\n",
    "# # ============================================================\n",
    "# def make_weights(r: int, alpha: float) -> np.ndarray:\n",
    "#     \"\"\"Exponential decay weights summing to 1: w_j for j=1..r.\"\"\"\n",
    "#     w = np.array([alpha ** (j - 1) for j in range(1, r + 1)], dtype=float)\n",
    "#     w /= w.sum()\n",
    "#     return w  # shape (r,)\n",
    "\n",
    "# def split_by_trajectory(df: pd.DataFrame, id_col: str, test_size: float, seed: int):\n",
    "#     \"\"\"Split train/test by whole trajectories (no leakage across same identifier).\"\"\"\n",
    "#     ids = df[id_col].dropna().unique()\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     rng.shuffle(ids)\n",
    "#     n_test = int(np.ceil(len(ids) * test_size))\n",
    "#     test_ids = set(ids[:n_test])\n",
    "#     train_mask = ~df[id_col].isin(test_ids)\n",
    "#     test_mask  = df[id_col].isin(test_ids)\n",
    "#     return df[train_mask].copy(), df[test_mask].copy()\n",
    "\n",
    "# def build_transition_matrix(train_df: pd.DataFrame, id_col: str, y_col: str, smooth: float = 1e-6):\n",
    "#     \"\"\"\n",
    "#     Build transition probability matrix P(next | prev) from training trajectories.\n",
    "#     Returns:\n",
    "#       T: dict prev_state -> (next_states array, probs array)\n",
    "#       all_states: sorted unique states\n",
    "#       prior: global prior over states (fallback)\n",
    "#     \"\"\"\n",
    "#     # All states (for fallback / smoothing)\n",
    "#     all_states = np.sort(train_df[y_col].dropna().unique()).astype(int)\n",
    "#     state_to_idx = {s: i for i, s in enumerate(all_states)}\n",
    "#     K = len(all_states)\n",
    "\n",
    "#     # Count transitions\n",
    "#     counts = np.zeros((K, K), dtype=float)  # counts[prev_idx, next_idx]\n",
    "#     # Also state prior (frequency)\n",
    "#     prior_counts = np.zeros(K, dtype=float)\n",
    "\n",
    "#     # Ensure correct order within each trajectory: assume merged CSV is already in time order.\n",
    "#     # If you have a timestamp column, sort within each group before counting.\n",
    "#     for tid, g in train_df.groupby(id_col, sort=False):\n",
    "#         seq = g[y_col].to_numpy()\n",
    "#         # drop NaNs\n",
    "#         seq = seq[~pd.isna(seq)].astype(int)\n",
    "#         if len(seq) == 0:\n",
    "#             continue\n",
    "#         for s in seq:\n",
    "#             if s in state_to_idx:\n",
    "#                 prior_counts[state_to_idx[s]] += 1\n",
    "#         for i in range(len(seq) - 1):\n",
    "#             a, b = seq[i], seq[i + 1]\n",
    "#             if a in state_to_idx and b in state_to_idx:\n",
    "#                 counts[state_to_idx[a], state_to_idx[b]] += 1\n",
    "\n",
    "#     # Smooth + row-normalize\n",
    "#     counts += smooth\n",
    "#     row_sums = counts.sum(axis=1, keepdims=True)\n",
    "#     probs = counts / row_sums\n",
    "\n",
    "#     # Build sparse dict view for speed\n",
    "#     T = {}\n",
    "#     for prev_idx, prev_state in enumerate(all_states):\n",
    "#         T[prev_state] = (all_states, probs[prev_idx])  # full support (simple + stable)\n",
    "\n",
    "#     # Global prior (fallback)\n",
    "#     prior_counts += smooth\n",
    "#     prior = prior_counts / prior_counts.sum()\n",
    "\n",
    "#     return T, all_states, prior\n",
    "\n",
    "# def train_knn_classifier(train_df: pd.DataFrame, x_cols, y_col: str):\n",
    "#     X = train_df[x_cols].to_numpy(dtype=float)\n",
    "#     y = train_df[y_col].to_numpy(dtype=int)\n",
    "\n",
    "#     scaler = None\n",
    "#     if USE_STANDARDIZE:\n",
    "#         scaler = StandardScaler()\n",
    "#         X = scaler.fit_transform(X)\n",
    "\n",
    "#     k = max(1, int(round(np.log(max(len(X), 2)))))  # safe log\n",
    "#     knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1, weights=\"distance\")\n",
    "#     knn.fit(X, y)\n",
    "#     return knn, scaler, k\n",
    "\n",
    "# def get_knn_proba(knn: KNeighborsClassifier, scaler, x_cols, df_row: pd.Series, all_states: np.ndarray):\n",
    "#     \"\"\"\n",
    "#     Return probability vector over all_states aligned to all_states.\n",
    "#     Uses knn.predict_proba and aligns class order.\n",
    "#     \"\"\"\n",
    "#     x = df_row[x_cols].to_numpy(dtype=float).reshape(1, -1)\n",
    "#     if scaler is not None:\n",
    "#         x = scaler.transform(x)\n",
    "\n",
    "#     proba = knn.predict_proba(x)[0]           # aligned to knn.classes_\n",
    "#     classes = knn.classes_.astype(int)\n",
    "\n",
    "#     # Map to full state space\n",
    "#     p_full = np.zeros(len(all_states), dtype=float)\n",
    "#     class_to_pos = {c: i for i, c in enumerate(classes)}\n",
    "#     for i, s in enumerate(all_states):\n",
    "#         if s in class_to_pos:\n",
    "#             p_full[i] = proba[class_to_pos[s]]\n",
    "#         else:\n",
    "#             p_full[i] = 0.0\n",
    "\n",
    "#     # Numerical safety\n",
    "#     ssum = p_full.sum()\n",
    "#     if ssum <= 0:\n",
    "#         p_full[:] = 1.0 / len(all_states)\n",
    "#     else:\n",
    "#         p_full /= ssum\n",
    "#     return p_full  # shape (K,)\n",
    "\n",
    "# def predict_trace_sequence(test_trace: pd.DataFrame,\n",
    "#                            knn, scaler,\n",
    "#                            x_cols, y_col, id_col,\n",
    "#                            T, all_states, prior,\n",
    "#                            r: int, weights: np.ndarray):\n",
    "#     \"\"\"\n",
    "#     Predict a sequence of location_id for one trajectory using:\n",
    "#       score(s) âˆ Pr(s|o_t) * Pr(s|H_t)\n",
    "#     where Pr(s|H_t) = sum_{j=1..r} w_j * Pr(s | s_{t-j})\n",
    "#     and Pr(s | s_prev) from transition matrix T.\n",
    "\n",
    "#     Returns:\n",
    "#       y_true (np.ndarray), y_pred (np.ndarray)\n",
    "#     \"\"\"\n",
    "#     y_true = test_trace[y_col].to_numpy(dtype=int)\n",
    "#     preds = []\n",
    "\n",
    "#     # Pre-extract transition rows for speed\n",
    "#     # T[prev] gives (all_states, probs_prev->*)\n",
    "#     def trans_from(prev_state: int):\n",
    "#         if prev_state in T:\n",
    "#             return T[prev_state][1]  # probs over all_states\n",
    "#         return prior  # fallback\n",
    "\n",
    "#     for t in range(len(test_trace)):\n",
    "#         p_obs = get_knn_proba(knn, scaler, x_cols, test_trace.iloc[t], all_states)\n",
    "\n",
    "#         if t == 0 or len(preds) == 0:\n",
    "#             # initial prediction: argmax Pr(s|o1)\n",
    "#             s_hat = all_states[int(np.argmax(p_obs))]\n",
    "#             preds.append(int(s_hat))\n",
    "#             continue\n",
    "\n",
    "#         # History-based transition mixture\n",
    "#         p_hist = np.zeros_like(prior, dtype=float)\n",
    "#         # use up to r previous predictions\n",
    "#         for j in range(1, r + 1):\n",
    "#             if t - j < 0:\n",
    "#                 break\n",
    "#             prev_pred = preds[t - j]\n",
    "#             wj = weights[j - 1]\n",
    "#             p_hist += wj * trans_from(prev_pred)\n",
    "\n",
    "#         # If we had less than r history, renormalize weights effectively used\n",
    "#         if t < r:\n",
    "#             w_used = weights[:t].sum()\n",
    "#             if w_used > 0:\n",
    "#                 p_hist /= w_used\n",
    "#             else:\n",
    "#                 p_hist = prior.copy()\n",
    "\n",
    "#         # Combine\n",
    "#         score = p_obs * p_hist\n",
    "#         if score.sum() <= 0:\n",
    "#             score = p_obs  # fallback\n",
    "#         s_hat = all_states[int(np.argmax(score))]\n",
    "#         preds.append(int(s_hat))\n",
    "\n",
    "#     return y_true, np.array(preds, dtype=int)\n",
    "\n",
    "# def bayes_risk_vs_trace_len(df: pd.DataFrame, name: str):\n",
    "#     \"\"\"\n",
    "#     1) Split by trajectories\n",
    "#     2) Train kNN on train\n",
    "#     3) Build transition matrix on train (true location_id sequences)\n",
    "#     4) For each L in MAX_TRACE_LEN_LIST, evaluate Bayes risk on test trajectories\n",
    "#        using first L points of each trajectory (if trajectory length >= L).\n",
    "#     \"\"\"\n",
    "#     # Basic checks\n",
    "#     for c in [ID_COL, Y_COL] + X_COLS:\n",
    "#         if c not in df.columns:\n",
    "#             raise ValueError(f\"[{name}] Missing required column: {c}\")\n",
    "\n",
    "#     # Split by trajectory to avoid leakage\n",
    "#     train_df, test_df = split_by_trajectory(df, ID_COL, TEST_SIZE, RANDOM_SEED)\n",
    "\n",
    "#     # Train classifier\n",
    "#     knn, scaler, k = train_knn_classifier(train_df, X_COLS, Y_COL)\n",
    "#     print(f\"[{name}] kNN k={k}, train_rows={len(train_df)}, test_rows={len(test_df)}\")\n",
    "\n",
    "#     # Transition matrix on training data\n",
    "#     T, all_states, prior = build_transition_matrix(train_df, ID_COL, Y_COL, smooth=LAPLACE_SMOOTH)\n",
    "\n",
    "#     weights = make_weights(HISTORY_R, ALPHA_DECAY)\n",
    "\n",
    "#     # Group test traces\n",
    "#     test_groups = list(test_df.groupby(ID_COL, sort=False))\n",
    "\n",
    "#     results = []\n",
    "#     for L in MAX_TRACE_LEN_LIST:\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         used_traces = 0\n",
    "\n",
    "#         for tid, g in test_groups:\n",
    "#             if len(g) < L:\n",
    "#                 continue\n",
    "#             used_traces += 1\n",
    "#             gL = g.iloc[:L]  # first L points in order\n",
    "\n",
    "#             y_true, y_pred = predict_trace_sequence(\n",
    "#                 gL, knn, scaler, X_COLS, Y_COL, ID_COL,\n",
    "#                 T, all_states, prior, HISTORY_R, weights\n",
    "#             )\n",
    "\n",
    "#             correct += int((y_true == y_pred).sum())\n",
    "#             total += len(y_true)\n",
    "\n",
    "#         if total == 0:\n",
    "#             results.append(np.nan)\n",
    "#         else:\n",
    "#             acc = correct / total\n",
    "#             risk = 1.0 - acc\n",
    "#             results.append(risk)\n",
    "\n",
    "#         print(f\"[{name}] L={L:>3}  traces_used={used_traces:>4}  points={total:>6}  BayesRisk={results[-1]}\")\n",
    "#     return results\n",
    "\n",
    "# # ============================================================\n",
    "# # RUN FOR PLM / PSM / PSMI\n",
    "# # ============================================================\n",
    "# def load_csv(path: str) -> pd.DataFrame:\n",
    "#     return pd.read_csv(path)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     df_plm  = load_csv(PLM_CSV)\n",
    "#     df_psm  = load_csv(PSM_CSV)\n",
    "#     df_psmi = load_csv(PSMI_CSV)\n",
    "\n",
    "#     risks_plm  = bayes_risk_vs_trace_len(df_plm,  \"PLM\")\n",
    "#     risks_psm  = bayes_risk_vs_trace_len(df_psm,  \"PSM\")\n",
    "#     risks_psmi = bayes_risk_vs_trace_len(df_psmi, \"PSM-I\")\n",
    "\n",
    "#     print(\"\\n==== FINAL LISTS (aligned with MAX_TRACE_LEN_LIST) ====\")\n",
    "#     print(\"trace_lengths =\", MAX_TRACE_LEN_LIST)\n",
    "#     print(\"plm  =\", risks_plm)\n",
    "#     print(\"psm  =\", risks_psm)\n",
    "#     print(\"psmi =\", risks_psmi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2cd83bc-4cc0-4a39-9809-3f11187dbdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # ============================================================\n",
    "# # EDIT THESE PATHS\n",
    "# # ============================================================\n",
    "# PLM_CSV  = r\"C:\\Users\\ss6365\\Desktop\\PrivAR_PSM_PSM_I\\data\\distorted_only_privacy\\geotrace\\delta_3_Noisy_threshold_5\\perturbed_1_copy_data_5\\plm\\grid40\\merged_geotrace_plm_delta_3_Noisy_threshold_5_eps05_grid40.csv\"\n",
    "# PSM_CSV  = r\"C:\\Users\\ss6365\\Desktop\\PrivAR_PSM_PSM_I\\data\\distorted_only_privacy\\geotrace\\delta_3_Noisy_threshold_5\\perturbed_1_copy_data_5\\psm\\grid40\\merged_geotrace_psm_delta_3_Noisy_threshold_5_eps05_grid40.csv\"\n",
    "# PSMI_CSV = r\"C:\\Users\\ss6365\\Desktop\\PrivAR_PSM_PSM_I\\data\\distorted_only_privacy\\geotrace\\delta_3_Noisy_threshold_5\\perturbed_1_copy_data_5\\psmi\\grid40\\merged_geotrace_psmi_delta_3_Noisy_threshold_5_eps05_grid40.csv\"\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # COLUMN NAMES (EDIT IF NEEDED)\n",
    "# # ============================================================\n",
    "# ID_COL = \"identifier\"  # tells which trace a row belongs to\n",
    "# X_COLS = [\"perturbed_latitude\", \"perturbed_longitude\"]  # attacker observes\n",
    "# Y_COL  = \"location_id\"  # attacker wants to predict\n",
    "\n",
    "# # ============================================================\n",
    "# # SETTINGS\n",
    "# # ============================================================\n",
    "# TEST_RATIO = 0.20\n",
    "# SEED = 42\n",
    "\n",
    "# TRACE_LENGTHS = [1, 5, 10, 20, 25]  # L values you want\n",
    "\n",
    "# # How many previous predicted states to use (history length)\n",
    "# # For trace length L, we use min(R, L-1) previous predictions\n",
    "# R = 10\n",
    "\n",
    "# # Transition smoothing (avoid zero-prob transitions)\n",
    "# SMOOTH = 1e-6\n",
    "\n",
    "# # Use distance-weighted kNN and standardize lat/lon (recommended)\n",
    "# USE_STANDARDIZE = True\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # HELPERS\n",
    "# # ============================================================\n",
    "# def split_by_identifier(df, id_col, test_ratio, seed):\n",
    "#     \"\"\"Split so that entire traces (identifiers) are either train or test (no leakage).\"\"\"\n",
    "#     ids = df[id_col].dropna().unique()\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     rng.shuffle(ids)\n",
    "#     n_test = int(np.ceil(len(ids) * test_ratio))\n",
    "#     test_ids = set(ids[:n_test])\n",
    "#     train_df = df[~df[id_col].isin(test_ids)].copy()\n",
    "#     test_df  = df[df[id_col].isin(test_ids)].copy()\n",
    "#     return train_df, test_df\n",
    "\n",
    "\n",
    "# def train_knn(train_df):\n",
    "#     X = train_df[X_COLS].to_numpy(dtype=float)\n",
    "#     y = train_df[Y_COL].to_numpy(dtype=int)\n",
    "\n",
    "#     scaler = None\n",
    "#     if USE_STANDARDIZE:\n",
    "#         scaler = StandardScaler()\n",
    "#         X = scaler.fit_transform(X)\n",
    "\n",
    "#     k = max(1, int(round(np.log(max(len(X), 2)))))\n",
    "#     knn = KNeighborsClassifier(n_neighbors=k, weights=\"distance\", n_jobs=-1)\n",
    "#     knn.fit(X, y)\n",
    "#     return knn, scaler, k\n",
    "\n",
    "\n",
    "# def knn_proba_full(knn, scaler, x_row, all_states):\n",
    "#     \"\"\"Return Pr(s|o) aligned to all_states.\"\"\"\n",
    "#     x = x_row.reshape(1, -1)\n",
    "#     if scaler is not None:\n",
    "#         x = scaler.transform(x)\n",
    "\n",
    "#     p = knn.predict_proba(x)[0]                # aligned to knn.classes_\n",
    "#     classes = knn.classes_.astype(int)\n",
    "\n",
    "#     p_full = np.zeros(len(all_states), dtype=float)\n",
    "#     cls_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "#     for i, s in enumerate(all_states):\n",
    "#         if s in cls_to_idx:\n",
    "#             p_full[i] = p[cls_to_idx[s]]\n",
    "\n",
    "#     ssum = p_full.sum()\n",
    "#     if ssum <= 0:\n",
    "#         p_full[:] = 1.0 / len(all_states)\n",
    "#     else:\n",
    "#         p_full /= ssum\n",
    "#     return p_full\n",
    "\n",
    "\n",
    "# def build_transition_probs(train_df):\n",
    "#     \"\"\"\n",
    "#     Build Pr(next_state | prev_state) from training TRUE location_id sequences.\n",
    "#     Output:\n",
    "#       all_states: sorted list of states\n",
    "#       P: dict prev_state -> prob vector over all_states\n",
    "#       prior: global prior over states (fallback)\n",
    "#     \"\"\"\n",
    "#     all_states = np.sort(train_df[Y_COL].dropna().unique()).astype(int)\n",
    "#     idx = {s: i for i, s in enumerate(all_states)}\n",
    "#     K = len(all_states)\n",
    "\n",
    "#     counts = np.zeros((K, K), dtype=float)\n",
    "#     prior_counts = np.zeros(K, dtype=float)\n",
    "\n",
    "#     for _, g in train_df.groupby(ID_COL, sort=False):\n",
    "#         seq = g[Y_COL].to_numpy()\n",
    "#         seq = seq[~pd.isna(seq)].astype(int)\n",
    "#         if len(seq) == 0:\n",
    "#             continue\n",
    "\n",
    "#         for s in seq:\n",
    "#             if s in idx:\n",
    "#                 prior_counts[idx[s]] += 1\n",
    "\n",
    "#         for t in range(len(seq) - 1):\n",
    "#             a, b = seq[t], seq[t + 1]\n",
    "#             if a in idx and b in idx:\n",
    "#                 counts[idx[a], idx[b]] += 1\n",
    "\n",
    "#     # Smooth and normalize\n",
    "#     counts += SMOOTH\n",
    "#     row_sums = counts.sum(axis=1, keepdims=True)\n",
    "#     probs = counts / row_sums\n",
    "\n",
    "#     prior_counts += SMOOTH\n",
    "#     prior = prior_counts / prior_counts.sum()\n",
    "\n",
    "#     P = {prev: probs[idx[prev]] for prev in all_states}\n",
    "#     return all_states, P, prior\n",
    "\n",
    "\n",
    "# def history_transition_mix(P, prior, all_states, prev_preds, r):\n",
    "#     \"\"\"\n",
    "#     Compute Pr(s | history) from previous predicted states.\n",
    "#     Here we use a simple decaying weight (recent history matters more).\n",
    "#     \"\"\"\n",
    "#     if len(prev_preds) == 0:\n",
    "#         return prior.copy()\n",
    "\n",
    "#     use = prev_preds[-r:]  # last r preds\n",
    "#     # exponential decay weights: most recent gets largest\n",
    "#     alpha = 0.7\n",
    "#     w = np.array([alpha**i for i in range(len(use))], dtype=float)  # i=0 is most recent\n",
    "#     w = w / w.sum()\n",
    "\n",
    "#     p_hist = np.zeros(len(all_states), dtype=float)\n",
    "#     # combine transitions from each previous predicted state\n",
    "#     for weight, prev_state in zip(w, reversed(use)):  # reversed so most recent aligns with i=0\n",
    "#         if prev_state in P:\n",
    "#             p_hist += weight * P[prev_state]\n",
    "#         else:\n",
    "#             p_hist += weight * prior\n",
    "\n",
    "#     # normalize\n",
    "#     ssum = p_hist.sum()\n",
    "#     if ssum <= 0:\n",
    "#         return prior.copy()\n",
    "#     return p_hist / ssum\n",
    "\n",
    "\n",
    "# def bayes_risk_vs_trace_length(df, name):\n",
    "#     # sanity\n",
    "#     for c in [ID_COL, Y_COL] + X_COLS:\n",
    "#         if c not in df.columns:\n",
    "#             raise ValueError(f\"[{name}] Missing column: {c}\")\n",
    "\n",
    "#     # split by trajectory\n",
    "#     train_df, test_df = split_by_identifier(df, ID_COL, TEST_RATIO, SEED)\n",
    "\n",
    "#     # train models\n",
    "#     knn, scaler, k = train_knn(train_df)\n",
    "#     all_states, P, prior = build_transition_probs(train_df)\n",
    "\n",
    "#     print(f\"[{name}] train_rows={len(train_df)} test_rows={len(test_df)}  kNN_k={k}  #states={len(all_states)}\")\n",
    "\n",
    "#     # group test trajectories (keep order as-is; assumes merged is time-ordered within identifier)\n",
    "#     test_groups = list(test_df.groupby(ID_COL, sort=False))\n",
    "\n",
    "#     risks = []\n",
    "#     for L in TRACE_LENGTHS:\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         used_traces = 0\n",
    "\n",
    "#         for _, g in test_groups:\n",
    "#             if len(g) < L:\n",
    "#                 continue\n",
    "#             used_traces += 1\n",
    "\n",
    "#             gL = g.iloc[:L]  # first L points\n",
    "#             X = gL[X_COLS].to_numpy(dtype=float)\n",
    "#             y_true = gL[Y_COL].to_numpy(dtype=int)\n",
    "\n",
    "#             prev_preds = []\n",
    "#             # sequentially predict 1..L\n",
    "#             for t in range(L):\n",
    "#                 p_obs = knn_proba_full(knn, scaler, X[t], all_states)\n",
    "\n",
    "#                 if t == 0:\n",
    "#                     # L=1 concept: argmax Pr(s|o1)\n",
    "#                     pred = int(all_states[np.argmax(p_obs)])\n",
    "#                 else:\n",
    "#                     # use history to improve predicting the current point\n",
    "#                     r = min(R, t)  # up to t previous predictions\n",
    "#                     p_hist = history_transition_mix(P, prior, all_states, prev_preds, r)\n",
    "#                     score = p_obs * p_hist\n",
    "#                     if score.sum() <= 0:\n",
    "#                         score = p_obs\n",
    "#                     pred = int(all_states[np.argmax(score)])\n",
    "\n",
    "#                 prev_preds.append(pred)\n",
    "\n",
    "#             # Evaluate ONLY the L-th prediction (index L-1)\n",
    "#             total += 1\n",
    "#             if prev_preds[L - 1] == y_true[L - 1]:\n",
    "#                 correct += 1\n",
    "\n",
    "#         acc = (correct / total) if total > 0 else np.nan\n",
    "#         risk = 1.0 - acc if total > 0 else np.nan\n",
    "#         risks.append(risk)\n",
    "\n",
    "#         print(f\"[{name}] L={L:>3} traces_used={used_traces:>4}  BayesRisk={risk}\")\n",
    "\n",
    "#     return risks\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # RUN\n",
    "# # ============================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     df_plm  = pd.read_csv(PLM_CSV)\n",
    "#     df_psm  = pd.read_csv(PSM_CSV)\n",
    "#     df_psmi = pd.read_csv(PSMI_CSV)\n",
    "\n",
    "#     risks_plm  = bayes_risk_vs_trace_length(df_plm,  \"PLM\")\n",
    "#     risks_psm  = bayes_risk_vs_trace_length(df_psm,  \"PSM\")\n",
    "#     risks_psmi = bayes_risk_vs_trace_length(df_psmi, \"PSM-I\")\n",
    "\n",
    "#     print(\"\\n==== FINAL OUTPUT ====\")\n",
    "#     print(\"trace_lengths =\", TRACE_LENGTHS)\n",
    "#     print(\"plm  =\", risks_plm)\n",
    "#     print(\"psm  =\", risks_psm)\n",
    "#     print(\"psmi =\", risks_psmi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcc4ddf2-350f-41c1-beed-767d60f97df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PLM] train_rows=249170 test_rows=69685 kNN_k_used=5 states=259\n",
      "[PLM] L=  1 traces_used=   8 BayesRisk=0.0\n",
      "[PLM] L=  2 traces_used=   8 BayesRisk=0.0\n",
      "[PLM] L=  3 traces_used=   8 BayesRisk=0.0\n",
      "[PLM] L=  5 traces_used=   8 BayesRisk=0.0\n",
      "[PLM] L= 10 traces_used=   8 BayesRisk=0.125\n",
      "[PLM] L= 20 traces_used=   8 BayesRisk=0.0\n",
      "[PLM] L= 30 traces_used=   8 BayesRisk=0.125\n",
      "[PSM] train_rows=249170 test_rows=69685 kNN_k_used=5 states=259\n",
      "[PSM] L=  1 traces_used=   8 BayesRisk=0.0\n",
      "[PSM] L=  2 traces_used=   8 BayesRisk=0.0\n",
      "[PSM] L=  3 traces_used=   8 BayesRisk=0.0\n",
      "[PSM] L=  5 traces_used=   8 BayesRisk=0.0\n",
      "[PSM] L= 10 traces_used=   8 BayesRisk=0.0\n",
      "[PSM] L= 20 traces_used=   8 BayesRisk=0.125\n",
      "[PSM] L= 30 traces_used=   8 BayesRisk=0.25\n",
      "[PSM-I] train_rows=249170 test_rows=69685 kNN_k_used=5 states=259\n",
      "[PSM-I] L=  1 traces_used=   8 BayesRisk=0.0\n",
      "[PSM-I] L=  2 traces_used=   8 BayesRisk=0.0\n",
      "[PSM-I] L=  3 traces_used=   8 BayesRisk=0.0\n",
      "[PSM-I] L=  5 traces_used=   8 BayesRisk=0.0\n",
      "[PSM-I] L= 10 traces_used=   8 BayesRisk=0.125\n",
      "[PSM-I] L= 20 traces_used=   8 BayesRisk=0.125\n",
      "[PSM-I] L= 30 traces_used=   8 BayesRisk=0.25\n",
      "\n",
      "==== FINAL LISTS ====\n",
      "trace_lengths = [1, 2, 3, 5, 10, 20, 30]\n",
      "plm  = [0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.125]\n",
      "psm  = [0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.25]\n",
      "psmi = [0.0, 0.0, 0.0, 0.0, 0.125, 0.125, 0.25]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # ============================================================\n",
    "# # EDIT THESE PATHS\n",
    "# # ============================================================\n",
    "# PLM_CSV  = r\"C:\\Users\\ss6365\\Desktop\\PrivAR_PSM_PSM_I\\data\\distorted_only_privacy\\geotrace\\delta_3_Noisy_threshold_5\\perturbed_1_copy_data_5\\plm\\grid40\\merged_geotrace_plm_delta_3_Noisy_threshold_5_eps05_grid40.csv\"\n",
    "# PSM_CSV  = r\"C:\\Users\\ss6365\\Desktop\\PrivAR_PSM_PSM_I\\data\\distorted_only_privacy\\geotrace\\delta_3_Noisy_threshold_5\\perturbed_1_copy_data_5\\psm\\grid40\\merged_geotrace_psm_delta_3_Noisy_threshold_5_eps05_grid40.csv\"\n",
    "# PSMI_CSV = r\"C:\\Users\\ss6365\\Desktop\\PrivAR_PSM_PSM_I\\data\\distorted_only_privacy\\geotrace\\delta_3_Noisy_threshold_5\\perturbed_1_copy_data_5\\psmi\\grid40\\merged_geotrace_psmi_delta_3_Noisy_threshold_5_eps05_grid40.csv\"\n",
    "\n",
    "# # ============================================================\n",
    "# # EDIT THESE COLUMN NAMES IF NEEDED\n",
    "# # ============================================================\n",
    "# ID_COL   = \"identifier\"   # trajectory/user id\n",
    "# TIME_COL = None           # e.g., \"timestamp\" if you have it; else keep None\n",
    "\n",
    "# X_COLS   = [\"perturbed_latitude\", \"perturbed_longitude\"]  # attacker observes these\n",
    "# Y_COL    = \"location_id\"                                 # attacker predicts this\n",
    "\n",
    "# # ============================================================\n",
    "# # SETTINGS YOU CONTROL\n",
    "# # ============================================================\n",
    "# KNN_K = 5           # <-- YOU choose k here\n",
    "# TEST_RATIO = 0.20\n",
    "# SEED = 42\n",
    "\n",
    "# TRACE_LENGTHS = [1, 2, 3, 5, 10, 20, 30]\n",
    "# R_HISTORY = 10       # use up to last R predicted states for history term\n",
    "# ALPHA_DECAY = 0.7    # exponential decay for history weights (recent states matter more)\n",
    "# SMOOTH = 1e-6        # smoothing for transition probabilities\n",
    "# USE_STANDARDIZE = True\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # UTILS\n",
    "# # ============================================================\n",
    "# def check_columns(df, required, name):\n",
    "#     missing = [c for c in required if c not in df.columns]\n",
    "#     if missing:\n",
    "#         raise ValueError(f\"[{name}] Missing columns: {missing}\\nAvailable columns: {list(df.columns)}\")\n",
    "\n",
    "# def split_by_identifier(df, id_col, test_ratio, seed):\n",
    "#     ids = df[id_col].dropna().unique()\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     rng.shuffle(ids)\n",
    "#     n_test = int(np.ceil(len(ids) * test_ratio))\n",
    "#     test_ids = set(ids[:n_test])\n",
    "#     train_df = df[~df[id_col].isin(test_ids)].copy()\n",
    "#     test_df  = df[df[id_col].isin(test_ids)].copy()\n",
    "#     return train_df, test_df\n",
    "\n",
    "# def safe_standardize_fit_transform(X):\n",
    "#     scaler = StandardScaler()\n",
    "#     return scaler, scaler.fit_transform(X)\n",
    "\n",
    "# def safe_standardize_transform(scaler, X):\n",
    "#     return scaler.transform(X)\n",
    "\n",
    "# def build_transition_probs(train_df):\n",
    "#     \"\"\"\n",
    "#     Build P(next | prev) from training TRUE location_id sequences.\n",
    "#     Returns:\n",
    "#       all_states: sorted states\n",
    "#       P: dict prev_state -> prob vector over all_states\n",
    "#       prior: global prior vector over all_states\n",
    "#     \"\"\"\n",
    "#     all_states = np.sort(train_df[Y_COL].dropna().unique()).astype(int)\n",
    "#     idx = {s: i for i, s in enumerate(all_states)}\n",
    "#     K = len(all_states)\n",
    "\n",
    "#     counts = np.zeros((K, K), dtype=float)\n",
    "#     prior_counts = np.zeros(K, dtype=float)\n",
    "\n",
    "#     for _, g in train_df.groupby(ID_COL, sort=False):\n",
    "#         if TIME_COL is not None and TIME_COL in g.columns:\n",
    "#             g = g.sort_values(TIME_COL, kind=\"stable\")\n",
    "\n",
    "#         seq = g[Y_COL].to_numpy()\n",
    "#         seq = seq[~pd.isna(seq)].astype(int)\n",
    "#         if len(seq) == 0:\n",
    "#             continue\n",
    "\n",
    "#         for s in seq:\n",
    "#             if s in idx:\n",
    "#                 prior_counts[idx[s]] += 1\n",
    "\n",
    "#         for t in range(len(seq) - 1):\n",
    "#             a, b = seq[t], seq[t + 1]\n",
    "#             if a in idx and b in idx:\n",
    "#                 counts[idx[a], idx[b]] += 1\n",
    "\n",
    "#     # smooth + normalize\n",
    "#     counts += SMOOTH\n",
    "#     probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "#     prior_counts += SMOOTH\n",
    "#     prior = prior_counts / prior_counts.sum()\n",
    "\n",
    "#     P = {prev: probs[idx[prev]] for prev in all_states}\n",
    "#     return all_states, P, prior\n",
    "\n",
    "# def history_mix(P, prior, all_states, prev_preds, r, alpha):\n",
    "#     \"\"\"\n",
    "#     Pr(s | history) = weighted mixture of P(s | prev_pred)\n",
    "#     with exponential decay weights (most recent gets highest weight).\n",
    "#     \"\"\"\n",
    "#     if len(prev_preds) == 0:\n",
    "#         return prior.copy()\n",
    "\n",
    "#     use = prev_preds[-r:]\n",
    "#     # weights for most recent first\n",
    "#     w = np.array([alpha**i for i in range(len(use))], dtype=float)  # i=0 most recent\n",
    "#     w /= w.sum()\n",
    "\n",
    "#     p_hist = np.zeros(len(all_states), dtype=float)\n",
    "#     # iterate most recent -> older\n",
    "#     for weight, prev_state in zip(w, reversed(use)):\n",
    "#         p_hist += weight * (P.get(prev_state, prior))\n",
    "\n",
    "#     ssum = p_hist.sum()\n",
    "#     return prior.copy() if ssum <= 0 else (p_hist / ssum)\n",
    "\n",
    "# def knn_proba_full(knn, scaler, x_row, all_states):\n",
    "#     \"\"\"\n",
    "#     Get Pr(s|o) aligned with all_states.\n",
    "#     \"\"\"\n",
    "#     x = x_row.reshape(1, -1)\n",
    "#     if scaler is not None:\n",
    "#         x = safe_standardize_transform(scaler, x)\n",
    "\n",
    "#     p = knn.predict_proba(x)[0]\n",
    "#     classes = knn.classes_.astype(int)\n",
    "\n",
    "#     p_full = np.zeros(len(all_states), dtype=float)\n",
    "#     cls_to_pos = {c: i for i, c in enumerate(classes)}\n",
    "#     for i, s in enumerate(all_states):\n",
    "#         j = cls_to_pos.get(s, None)\n",
    "#         if j is not None:\n",
    "#             p_full[i] = p[j]\n",
    "\n",
    "#     ssum = p_full.sum()\n",
    "#     if ssum <= 0:\n",
    "#         p_full[:] = 1.0 / len(all_states)\n",
    "#     else:\n",
    "#         p_full /= ssum\n",
    "#     return p_full\n",
    "\n",
    "# def train_knn(train_df, k_user):\n",
    "#     X = train_df[X_COLS].to_numpy(dtype=float)\n",
    "#     y = train_df[Y_COL].to_numpy(dtype=int)\n",
    "\n",
    "#     # drop rows with NaNs in X or y\n",
    "#     mask = np.isfinite(X).all(axis=1) & np.isfinite(y)\n",
    "#     X, y = X[mask], y[mask]\n",
    "\n",
    "#     if len(X) == 0:\n",
    "#         raise ValueError(\"Training data became empty after removing NaNs.\")\n",
    "\n",
    "#     scaler = None\n",
    "#     if USE_STANDARDIZE:\n",
    "#         scaler, X = safe_standardize_fit_transform(X)\n",
    "\n",
    "#     # IMPORTANT: k must be <= number of training samples\n",
    "#     k = int(k_user)\n",
    "#     k = max(1, min(k, len(X)))\n",
    "\n",
    "#     knn = KNeighborsClassifier(n_neighbors=k, weights=\"distance\", n_jobs=-1)\n",
    "#     knn.fit(X, y)\n",
    "#     return knn, scaler, k\n",
    "\n",
    "# def bayes_risk_vs_L(df, name):\n",
    "#     check_columns(df, [ID_COL, Y_COL] + X_COLS, name)\n",
    "\n",
    "#     # Split by trajectory (no leakage)\n",
    "#     train_df, test_df = split_by_identifier(df, ID_COL, TEST_RATIO, SEED)\n",
    "\n",
    "#     # Optional: sort within each identifier if time exists\n",
    "#     if TIME_COL is not None and TIME_COL in df.columns:\n",
    "#         train_df = train_df.sort_values([ID_COL, TIME_COL], kind=\"stable\")\n",
    "#         test_df  = test_df.sort_values([ID_COL, TIME_COL], kind=\"stable\")\n",
    "\n",
    "#     # Train kNN + build transitions\n",
    "#     knn, scaler, k_used = train_knn(train_df, KNN_K)\n",
    "#     all_states, P, prior = build_transition_probs(train_df)\n",
    "\n",
    "#     print(f\"[{name}] train_rows={len(train_df)} test_rows={len(test_df)} kNN_k_used={k_used} states={len(all_states)}\")\n",
    "\n",
    "#     risks = []\n",
    "#     groups = list(test_df.groupby(ID_COL, sort=False))\n",
    "\n",
    "#     for L in TRACE_LENGTHS:\n",
    "#         correct_L = 0\n",
    "#         total_L = 0\n",
    "#         used_traces = 0\n",
    "\n",
    "#         for _, g in groups:\n",
    "#             if len(g) < L:\n",
    "#                 continue\n",
    "#             used_traces += 1\n",
    "\n",
    "#             gL = g.iloc[:L]\n",
    "#             X = gL[X_COLS].to_numpy(dtype=float)\n",
    "#             y_true = gL[Y_COL].to_numpy(dtype=int)\n",
    "\n",
    "#             # sequential prediction 1..L\n",
    "#             prev_preds = []\n",
    "#             for t in range(L):\n",
    "#                 p_obs = knn_proba_full(knn, scaler, X[t], all_states)\n",
    "\n",
    "#                 if t == 0:\n",
    "#                     pred = int(all_states[np.argmax(p_obs)])\n",
    "#                 else:\n",
    "#                     r = min(R_HISTORY, t)\n",
    "#                     p_hist = history_mix(P, prior, all_states, prev_preds, r, ALPHA_DECAY)\n",
    "#                     score = p_obs * p_hist\n",
    "#                     if score.sum() <= 0:\n",
    "#                         score = p_obs\n",
    "#                     pred = int(all_states[np.argmax(score)])\n",
    "\n",
    "#                 prev_preds.append(pred)\n",
    "\n",
    "#             # Evaluate ONLY the L-th point (your exact concept)\n",
    "#             total_L += 1\n",
    "#             if prev_preds[L - 1] == y_true[L - 1]:\n",
    "#                 correct_L += 1\n",
    "\n",
    "#         acc = (correct_L / total_L) if total_L > 0 else np.nan\n",
    "#         risk = 1.0 - acc if total_L > 0 else np.nan\n",
    "#         risks.append(risk)\n",
    "#         print(f\"[{name}] L={L:>3} traces_used={used_traces:>4} BayesRisk={risk}\")\n",
    "\n",
    "#     return risks\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # RUN\n",
    "# # ============================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     df_plm  = pd.read_csv(PLM_CSV)\n",
    "#     df_psm  = pd.read_csv(PSM_CSV)\n",
    "#     df_psmi = pd.read_csv(PSMI_CSV)\n",
    "\n",
    "#     risks_plm  = bayes_risk_vs_L(df_plm,  \"PLM\")\n",
    "#     risks_psm  = bayes_risk_vs_L(df_psm,  \"PSM\")\n",
    "#     risks_psmi = bayes_risk_vs_L(df_psmi, \"PSM-I\")\n",
    "\n",
    "#     print(\"\\n==== FINAL LISTS ====\")\n",
    "#     print(\"trace_lengths =\", TRACE_LENGTHS)\n",
    "#     print(\"plm  =\", risks_plm)\n",
    "#     print(\"psm  =\", risks_psm)\n",
    "#     print(\"psmi =\", risks_psmi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ecfb17-0c36-4740-8dee-a03cbfdc29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =========================\n",
    "# CONFIG (EDIT)\n",
    "# =========================\n",
    "KNN_K = 5                 # YOU choose k\n",
    "NUM_RUNS = 1            # like your num_runs (averaging randomness)\n",
    "TEST_RATIO = 0.20\n",
    "SEED0 = 42\n",
    "\n",
    "TRACE_LENGTHS = [1,  5, 10, 20]   # evaluate these L values\n",
    "\n",
    "ID_COL = \"identifier\"\n",
    "X_COLS = [\"perturbed_latitude\", \"perturbed_longitude\"]\n",
    "Y_COL  = \"location_id\"\n",
    "\n",
    "USE_STANDARDIZE = True\n",
    "SMOOTH = 1e-6             # smoothing for transition matrix\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def split_by_identifier(df, id_col, test_ratio, seed):\n",
    "    ids = df[id_col].dropna().unique()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(ids)\n",
    "    n_test = int(np.ceil(len(ids) * test_ratio))\n",
    "    test_ids = set(ids[:n_test])\n",
    "    train_df = df[~df[id_col].isin(test_ids)].copy()\n",
    "    test_df  = df[df[id_col].isin(test_ids)].copy()\n",
    "    return train_df, test_df\n",
    "\n",
    "def train_knn(train_df, k_user):\n",
    "    X = train_df[X_COLS].to_numpy(dtype=float)\n",
    "    y = train_df[Y_COL].to_numpy(dtype=int)\n",
    "\n",
    "    # drop NaNs\n",
    "    ok = np.isfinite(X).all(axis=1) & np.isfinite(y)\n",
    "    X, y = X[ok], y[ok]\n",
    "\n",
    "    scaler = None\n",
    "    if USE_STANDARDIZE:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "    k = int(k_user)\n",
    "    k = max(1, min(k, len(X)))  # prevent k > #train_samples\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1, weights=\"distance\")\n",
    "    knn.fit(X, y)\n",
    "    return knn, scaler, k\n",
    "\n",
    "def knn_proba_full(knn, scaler, x_row, all_states):\n",
    "    x = x_row.reshape(1, -1)\n",
    "    if scaler is not None:\n",
    "        x = scaler.transform(x)\n",
    "\n",
    "    p = knn.predict_proba(x)[0]\n",
    "    classes = knn.classes_.astype(int)\n",
    "\n",
    "    p_full = np.zeros(len(all_states), dtype=float)\n",
    "    cls_to_pos = {c: i for i, c in enumerate(classes)}\n",
    "    for i, s in enumerate(all_states):\n",
    "        j = cls_to_pos.get(s, None)\n",
    "        if j is not None:\n",
    "            p_full[i] = p[j]\n",
    "\n",
    "    ssum = p_full.sum()\n",
    "    if ssum <= 0:\n",
    "        p_full[:] = 1.0 / len(all_states)\n",
    "    else:\n",
    "        p_full /= ssum\n",
    "    return p_full\n",
    "\n",
    "def build_transition_matrix(train_df):\n",
    "    \"\"\"\n",
    "    First-order Markov transition: P(next | prev)\n",
    "    learned from TRAIN traces.\n",
    "    \"\"\"\n",
    "    all_states = np.sort(train_df[Y_COL].dropna().unique()).astype(int)\n",
    "    idx = {s: i for i, s in enumerate(all_states)}\n",
    "    K = len(all_states)\n",
    "\n",
    "    counts = np.zeros((K, K), dtype=float)\n",
    "\n",
    "    for _, g in train_df.groupby(ID_COL, sort=False):\n",
    "        seq = g[Y_COL].to_numpy()\n",
    "        seq = seq[~pd.isna(seq)].astype(int)\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "        for t in range(len(seq) - 1):\n",
    "            a, b = seq[t], seq[t + 1]\n",
    "            if a in idx and b in idx:\n",
    "                counts[idx[a], idx[b]] += 1\n",
    "\n",
    "    counts += SMOOTH\n",
    "    probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # dict: prev_state -> prob vector over all_states\n",
    "    P = {prev: probs[idx[prev]] for prev in all_states}\n",
    "    return all_states, P\n",
    "\n",
    "def sequential_predict_prefix(gL, knn, scaler, all_states, P):\n",
    "    \"\"\"\n",
    "    Predict s1..sL sequentially:\n",
    "      - t=1: argmax Pr(s|o1)\n",
    "      - t>=2: argmax Pr(s|ot) * P(s | s_hat_{t-1})\n",
    "    Return predicted list length L.\n",
    "    \"\"\"\n",
    "    X = gL[X_COLS].to_numpy(dtype=float)\n",
    "    preds = []\n",
    "\n",
    "    for t in range(len(gL)):\n",
    "        p_obs = knn_proba_full(knn, scaler, X[t], all_states)\n",
    "\n",
    "        if t == 0:\n",
    "            pred = int(all_states[np.argmax(p_obs)])\n",
    "        else:\n",
    "            prev = preds[t - 1]\n",
    "            p_tr = P.get(prev, np.ones(len(all_states)) / len(all_states))\n",
    "            score = p_obs * p_tr\n",
    "            if score.sum() <= 0:\n",
    "                score = p_obs\n",
    "            pred = int(all_states[np.argmax(score)])\n",
    "\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def bayes_risk_vs_trace_length(df, name):\n",
    "    # quick column check\n",
    "    for c in [ID_COL, Y_COL] + X_COLS:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"[{name}] Missing column '{c}'. Available: {list(df.columns)}\")\n",
    "\n",
    "    # store risks for each L across runs\n",
    "    risk_runs = {L: [] for L in TRACE_LENGTHS}\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "        seed = SEED0 + run\n",
    "\n",
    "        # split by identifier (no leakage)\n",
    "        train_df, test_df = split_by_identifier(df, ID_COL, TEST_RATIO, seed)\n",
    "\n",
    "        # train kNN + transitions\n",
    "        knn, scaler, k_used = train_knn(train_df, KNN_K)\n",
    "        all_states, P = build_transition_matrix(train_df)\n",
    "\n",
    "        # group test traces\n",
    "        groups = list(test_df.groupby(ID_COL, sort=False))\n",
    "\n",
    "        for L in TRACE_LENGTHS:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for _, g in groups:\n",
    "                if len(g) < L:\n",
    "                    continue\n",
    "\n",
    "                gL = g.iloc[:L]              # first L points\n",
    "                y_true_L = int(gL[Y_COL].iloc[L - 1])\n",
    "\n",
    "                preds = sequential_predict_prefix(gL, knn, scaler, all_states, P)\n",
    "                y_pred_L = preds[L - 1]\n",
    "\n",
    "                total += 1\n",
    "                if y_pred_L == y_true_L:\n",
    "                    correct += 1\n",
    "\n",
    "            if total == 0:\n",
    "                risk_runs[L].append(np.nan)\n",
    "            else:\n",
    "                acc = correct / total\n",
    "                risk_runs[L].append((1 - acc) * 100.0)\n",
    "\n",
    "    # average across runs\n",
    "    risks_avg = []\n",
    "    for L in TRACE_LENGTHS:\n",
    "        arr = np.array(risk_runs[L], dtype=float)\n",
    "        risks_avg.append(float(np.nanmean(arr)))\n",
    "\n",
    "    print(f\"[{name}] done. (k={KNN_K}, runs={NUM_RUNS})\")\n",
    "    return risks_avg\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RUN FOR THREE MECHANISMS\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these with your merged CSV paths\n",
    "    PLM_PATH  = r\"D:\\backup\\location_privacy_final\\collected\\machine_learning\\attack1\\laplace\\200\\merged_laplace_0.1_encoded_200.csv\"\n",
    "    #PSM_PATH  = r\"D:\\backup\\location_privacy_final\\collected\\machine_learning\\attack1\\laplace\\400\\merged_laplace_0.1_encoded_400.csv\"\n",
    "    PSMI_PATH = r\"D:\\backup\\location_privacy_final\\collected\\machine_learning\\attack1\\our_bl_50_delta_5\\200\\merged_our_bl_50_delta_5_0.1_encoded_200.csv\"\n",
    "\n",
    "    df_plm  = pd.read_csv(PLM_PATH)\n",
    "    #df_psm  = pd.read_csv(PSM_PATH)\n",
    "    df_psmi = pd.read_csv(PSMI_PATH)\n",
    "\n",
    "    risks_plm  = bayes_risk_vs_trace_length(df_plm,  \"PLM\")\n",
    "    #risks_psm  = bayes_risk_vs_trace_length(df_psm,  \"PSM\")\n",
    "    risks_psmi = bayes_risk_vs_trace_length(df_psmi, \"PSM-I\")\n",
    "\n",
    "    print(\"\\ntrace_lengths =\", TRACE_LENGTHS)\n",
    "    print(\"plm  =\", risks_plm)\n",
    "    #print(\"psm  =\", risks_psm)\n",
    "    print(\"psmi =\", risks_psmi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1872c2af-d24f-4931-b9a0-45f71aeac90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PLM] run=0 selected_k=31\n",
      "[PLM] run=1 selected_k=39\n",
      "[PLM] run=2 selected_k=39\n",
      "[PLM] run=3 selected_k=17\n",
      "[PLM] run=4 selected_k=17\n",
      "[PLM] run=5 selected_k=9\n",
      "[PLM] run=6 selected_k=33\n",
      "[PLM] run=7 selected_k=21\n",
      "[PLM] run=8 selected_k=31\n",
      "[PLM] run=9 selected_k=35\n",
      "[PLM] run=10 selected_k=43\n",
      "[PLM] run=11 selected_k=37\n",
      "[PLM] run=12 selected_k=21\n",
      "[PLM] run=13 selected_k=27\n",
      "[PLM] run=14 selected_k=45\n",
      "[PLM] run=15 selected_k=19\n",
      "[PLM] run=16 selected_k=33\n",
      "[PLM] run=17 selected_k=43\n",
      "[PLM] run=18 selected_k=15\n",
      "[PLM] run=19 selected_k=33\n",
      "[PLM] done. runs=20\n",
      "[PSM-I] run=0 selected_k=29\n",
      "[PSM-I] run=1 selected_k=45\n",
      "[PSM-I] run=2 selected_k=51\n",
      "[PSM-I] run=3 selected_k=49\n",
      "[PSM-I] run=4 selected_k=51\n",
      "[PSM-I] run=5 selected_k=23\n",
      "[PSM-I] run=6 selected_k=41\n",
      "[PSM-I] run=7 selected_k=51\n",
      "[PSM-I] run=8 selected_k=45\n",
      "[PSM-I] run=9 selected_k=51\n",
      "[PSM-I] run=10 selected_k=41\n",
      "[PSM-I] run=11 selected_k=35\n",
      "[PSM-I] run=12 selected_k=51\n",
      "[PSM-I] run=13 selected_k=35\n",
      "[PSM-I] run=14 selected_k=35\n",
      "[PSM-I] run=15 selected_k=37\n",
      "[PSM-I] run=16 selected_k=39\n",
      "[PSM-I] run=17 selected_k=51\n",
      "[PSM-I] run=18 selected_k=51\n",
      "[PSM-I] run=19 selected_k=27\n",
      "[PSM-I] done. runs=20\n",
      "\n",
      "trace_lengths = [1, 5, 10, 20]\n",
      "plm  = [23.571428571428573, 30.00000000000001, 32.85714285714287, 35.71428571428571]\n",
      "psmi = [60.000000000000014, 62.14285714285713, 63.571428571428555, 77.14285714285712]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =========================\n",
    "# CONFIG (EDIT)\n",
    "# =========================\n",
    "NUM_RUNS   = 20\n",
    "TEST_RATIO = 0.20\n",
    "SEED0      = 42\n",
    "\n",
    "TRACE_LENGTHS = [1, 5, 10, 20]\n",
    "\n",
    "ID_COL = \"identifier\"\n",
    "X_COLS = [\"perturbed_latitude\", \"perturbed_longitude\"]\n",
    "Y_COL  = \"location_id\"\n",
    "\n",
    "USE_STANDARDIZE = True\n",
    "SMOOTH = 1e-6  # smoothing for transition matrix\n",
    "\n",
    "# Auto-k selection settings\n",
    "VAL_RATIO = 0.20\n",
    "K_CANDIDATES = list(range(1, 52, 2))  # odd k's 1..51\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def split_by_identifier(df, id_col, test_ratio, seed):\n",
    "    ids = df[id_col].dropna().unique()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(ids)\n",
    "    n_test = int(np.ceil(len(ids) * test_ratio))\n",
    "    test_ids = set(ids[:n_test])\n",
    "    train_df = df[~df[id_col].isin(test_ids)].copy()\n",
    "    test_df  = df[df[id_col].isin(test_ids)].copy()\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def make_xy(df):\n",
    "    X = df[X_COLS].to_numpy(dtype=float)\n",
    "    y = df[Y_COL].to_numpy(dtype=int)\n",
    "    ok = np.isfinite(X).all(axis=1) & np.isfinite(y)\n",
    "    return X[ok], y[ok]\n",
    "\n",
    "\n",
    "def train_knn_auto_k(train_df, id_col, seed):\n",
    "    \"\"\"\n",
    "    Select k via validation split by identifier (no leakage), then train final kNN on full train_df.\n",
    "    Returns: knn, scaler, best_k\n",
    "    \"\"\"\n",
    "    # ---- sub-train / val split by identifier ----\n",
    "    ids = train_df[id_col].dropna().unique()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(ids)\n",
    "    n_val = int(np.ceil(len(ids) * VAL_RATIO))\n",
    "    val_ids = set(ids[:n_val])\n",
    "\n",
    "    sub_train = train_df[~train_df[id_col].isin(val_ids)].copy()\n",
    "    val_df    = train_df[train_df[id_col].isin(val_ids)].copy()\n",
    "\n",
    "    X_tr, y_tr = make_xy(sub_train)\n",
    "    X_va, y_va = make_xy(val_df)\n",
    "\n",
    "    if len(X_tr) == 0 or len(X_va) == 0:\n",
    "        raise ValueError(\"Empty sub-train or val after NaN filtering. Check data/columns.\")\n",
    "\n",
    "    scaler = None\n",
    "    if USE_STANDARDIZE:\n",
    "        scaler = StandardScaler()\n",
    "        X_tr = scaler.fit_transform(X_tr)\n",
    "        X_va = scaler.transform(X_va)\n",
    "\n",
    "    # clamp candidates so k <= #sub_train\n",
    "    max_k = len(X_tr)\n",
    "    k_candidates = [k for k in K_CANDIDATES if k <= max_k]\n",
    "    if not k_candidates:\n",
    "        k_candidates = [max(1, min(5, max_k))]\n",
    "\n",
    "    best_k = k_candidates[0]\n",
    "    best_acc = -1.0\n",
    "\n",
    "    for k in k_candidates:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1, weights=\"distance\")\n",
    "        knn.fit(X_tr, y_tr)\n",
    "        acc = float((knn.predict(X_va) == y_va).mean())\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_k = k\n",
    "\n",
    "    # ---- retrain final model on full train_df with best_k ----\n",
    "    X_full, y_full = make_xy(train_df)\n",
    "\n",
    "    scaler_full = None\n",
    "    if USE_STANDARDIZE:\n",
    "        scaler_full = StandardScaler()\n",
    "        X_full = scaler_full.fit_transform(X_full)\n",
    "\n",
    "    best_k = max(1, min(best_k, len(X_full)))\n",
    "    knn_final = KNeighborsClassifier(n_neighbors=best_k, n_jobs=-1, weights=\"distance\")\n",
    "    knn_final.fit(X_full, y_full)\n",
    "\n",
    "    return knn_final, scaler_full, best_k\n",
    "\n",
    "\n",
    "def knn_proba_full(knn, scaler, x_row, all_states):\n",
    "    x = x_row.reshape(1, -1)\n",
    "    if scaler is not None:\n",
    "        x = scaler.transform(x)\n",
    "\n",
    "    p = knn.predict_proba(x)[0]\n",
    "    classes = knn.classes_.astype(int)\n",
    "\n",
    "    p_full = np.zeros(len(all_states), dtype=float)\n",
    "    cls_to_pos = {c: i for i, c in enumerate(classes)}\n",
    "    for i, s in enumerate(all_states):\n",
    "        j = cls_to_pos.get(s, None)\n",
    "        if j is not None:\n",
    "            p_full[i] = p[j]\n",
    "\n",
    "    ssum = p_full.sum()\n",
    "    if ssum <= 0:\n",
    "        p_full[:] = 1.0 / len(all_states)\n",
    "    else:\n",
    "        p_full /= ssum\n",
    "    return p_full\n",
    "\n",
    "\n",
    "def build_transition_matrix(train_df):\n",
    "    all_states = np.sort(train_df[Y_COL].dropna().unique()).astype(int)\n",
    "    idx = {s: i for i, s in enumerate(all_states)}\n",
    "    K = len(all_states)\n",
    "\n",
    "    counts = np.zeros((K, K), dtype=float)\n",
    "\n",
    "    for _, g in train_df.groupby(ID_COL, sort=False):\n",
    "        seq = g[Y_COL].to_numpy()\n",
    "        seq = seq[~pd.isna(seq)].astype(int)\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "        for t in range(len(seq) - 1):\n",
    "            a, b = seq[t], seq[t + 1]\n",
    "            if a in idx and b in idx:\n",
    "                counts[idx[a], idx[b]] += 1\n",
    "\n",
    "    counts += SMOOTH\n",
    "    probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "    P = {prev: probs[idx[prev]] for prev in all_states}\n",
    "    return all_states, P\n",
    "\n",
    "\n",
    "def sequential_predict_prefix(gL, knn, scaler, all_states, P):\n",
    "    X = gL[X_COLS].to_numpy(dtype=float)\n",
    "    preds = []\n",
    "\n",
    "    for t in range(len(gL)):\n",
    "        p_obs = knn_proba_full(knn, scaler, X[t], all_states)\n",
    "\n",
    "        if t == 0:\n",
    "            pred = int(all_states[np.argmax(p_obs)])\n",
    "        else:\n",
    "            prev = preds[t - 1]\n",
    "            p_tr = P.get(prev, np.ones(len(all_states)) / len(all_states))\n",
    "            score = p_obs * p_tr\n",
    "            if score.sum() <= 0:\n",
    "                score = p_obs\n",
    "            pred = int(all_states[np.argmax(score)])\n",
    "\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def bayes_risk_vs_trace_length(df, name):\n",
    "    for c in [ID_COL, Y_COL] + X_COLS:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"[{name}] Missing column '{c}'. Available: {list(df.columns)}\")\n",
    "\n",
    "    risk_runs = {L: [] for L in TRACE_LENGTHS}\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "        seed = SEED0 + run\n",
    "\n",
    "        train_df, test_df = split_by_identifier(df, ID_COL, TEST_RATIO, seed)\n",
    "\n",
    "        # AUTO-select k here\n",
    "        knn, scaler, k_used = train_knn_auto_k(train_df, ID_COL, seed)\n",
    "        all_states, P = build_transition_matrix(train_df)\n",
    "\n",
    "        groups = list(test_df.groupby(ID_COL, sort=False))\n",
    "\n",
    "        for L in TRACE_LENGTHS:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for _, g in groups:\n",
    "                if len(g) < L:\n",
    "                    continue\n",
    "\n",
    "                gL = g.iloc[:L]\n",
    "                y_true_L = int(gL[Y_COL].iloc[L - 1])\n",
    "\n",
    "                preds = sequential_predict_prefix(gL, knn, scaler, all_states, P)\n",
    "                y_pred_L = preds[L - 1]\n",
    "\n",
    "                total += 1\n",
    "                if y_pred_L == y_true_L:\n",
    "                    correct += 1\n",
    "\n",
    "            if total == 0:\n",
    "                risk_runs[L].append(np.nan)\n",
    "            else:\n",
    "                acc = correct / total\n",
    "                risk_runs[L].append((1 - acc) * 100.0)\n",
    "\n",
    "        print(f\"[{name}] run={run} selected_k={k_used}\")\n",
    "\n",
    "    risks_avg = []\n",
    "    for L in TRACE_LENGTHS:\n",
    "        arr = np.array(risk_runs[L], dtype=float)\n",
    "        risks_avg.append(float(np.nanmean(arr)))\n",
    "\n",
    "    print(f\"[{name}] done. runs={NUM_RUNS}\")\n",
    "    return risks_avg\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RUN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    PLM_PATH  = r\"D:\\backup\\location_privacy_final\\collected\\machine_learning\\attack50\\laplace\\400\\merged_laplace_0.1_encoded_400.csv\"\n",
    "    PSMI_PATH = r\"D:\\backup\\location_privacy_final\\collected\\machine_learning\\attack50\\our_bl_50_delta_5\\400\\merged_our_bl_50_delta_5_0.1_encoded_400.csv\"\n",
    "\n",
    "    df_plm  = pd.read_csv(PLM_PATH)\n",
    "    df_psmi = pd.read_csv(PSMI_PATH)\n",
    "\n",
    "    risks_plm  = bayes_risk_vs_trace_length(df_plm,  \"PLM\")\n",
    "    risks_psmi = bayes_risk_vs_trace_length(df_psmi, \"PSM-I\")\n",
    "\n",
    "    print(\"\\ntrace_lengths =\", TRACE_LENGTHS)\n",
    "    print(\"plm  =\", risks_plm)\n",
    "    print(\"psmi =\", risks_psmi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b653c1-cdea-424e-9b1c-876321cb0c20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
